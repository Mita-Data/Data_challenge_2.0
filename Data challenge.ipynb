{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e1def575\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Data Challenge - Advanced Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"**The data challenge is proposed by Idemia**\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Description of the data challenge\\n\",\n",
    "    \"\\n\",\n",
    "    \"You have at your disposal 100000 images of human faces, and their occlusion label.\\n\",\n",
    "    \"The goal of this challenge is to regress the percentage of the face that is occluded.\\n\",\n",
    "    \"We also want to have similar performances on female and male, the gender label is given for the train database.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Below is the formula of the evaluation score. You'll first need to compute the error made by your model for men and women separetely. The error is measured as follows\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \" Err = \\\\frac{\\\\sum_{i}{w_i(p_i - GT_i)^2}}{\\\\sum_{i}{w_i}}, w_i = \\\\frac{1}{30} + GT_i,\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"where $GT_i$ is the ground truth and p_i the prediction made by your model. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Then, the final score for the leaderboard is given by\\n\",\n",
    "    \"\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"Score = \\\\frac{Err_F + Err_M}{2} + \\\\left | Err_F - Err_M \\\\right |,\\n\",\n",
    "    \"$$\\n\",\n",
    "    \"where $Err_F$ and $Err_M$ are the errors obtained for female and men, respectively.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Hereafter we show you how to load the data and run a naive baseline using a pretrained model.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"attachments\": {},\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6d771bd4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Downloading the Data:\\n\",\n",
    "    \"\\n\",\n",
    "    \"You can download the Dataset from the below links: https://partage.imt.fr/index.php/s/mscQABX4oZxx7ax\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"14a84fac-1388-46c0-8b2b-c5da6383b941\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries to run the naive baseline\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"from tqdm import tqdm\\n\",\n",
    "    \"from collections import OrderedDict\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torchvision\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"from torchvision.models import mobilenet_v3_small\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"cfb931c0-cd9d-4b23-8d96-1cfb9706c3de\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Load dataframes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"8e8dbe42-0b3e-4649-8aa8-2c2ce3b8d7d9\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_train = pd.read_csv(\\\"listes_training/data_100K/train_100K.csv\\\", delimiter=' ')\\n\",\n",
    "    \"df_test = pd.read_csv(\\\"listes_training/data_100K/test_students.csv\\\", delimiter=' ')\\n\",\n",
    "    \"\\n\",\n",
    "    \"image_dir = \\\"crops_100K\\\"\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"98b7e12a-f0f1-4779-9205-f4b7d44b84a1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_train.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"18b973ad-10c6-44fd-bc23-3fbb644f12d7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_test.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e2225ac6-a74d-43f7-a5a2-dae2a8bd5728\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Remove nan values\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"4aaef2a2-4512-460b-af97-f91cece13caf\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_train = df_train.dropna()\\n\",\n",
    "    \"df_test = df_test.dropna()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7b6835d8-2964-4822-bd1c-e15fba297e42\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Split Dataframe in train and val\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"34261442-22d9-4611-9a65-1ccc5fbc4bff\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"df_val = df_train.loc[:20000].reset_index()\\n\",\n",
    "    \"df_train = df_train.loc[20000:].reset_index()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"309660aa-d8a9-404a-b7d7-5f4bb5dac72a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"len(df_train), len(df_val), len(df_test)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"925365d9-57c3-40e6-afe6-f555c9ebaa7c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Check that all images are read correctly\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a13e82aa-db54-4961-b391-ea7d4721a377\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"for idx, row in tqdm(df_train.iterrows(), total=len(df_train)):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        filename = df_train.loc[idx, 'filename']\\n\",\n",
    "    \"        img2display = Image.open(f\\\"{image_dir}/{filename}\\\")\\n\",\n",
    "    \"    except ValueError as e:\\n\",\n",
    "    \"        print(idx, e)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"for idx, row in tqdm(df_test.iterrows(), total=len(df_test)):\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        filename = df_test.loc[idx, 'filename']\\n\",\n",
    "    \"        img2display = Image.open(f\\\"{image_dir}/{filename}\\\")\\n\",\n",
    "    \"    except ValueError as e:\\n\",\n",
    "    \"        print(idx, e)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1f0f4aaa-05d3-4c47-a01e-415781e4ba72\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Display first images\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a6518d04-f6e5-4d5c-a51e-b34711825f54\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"for idx, row in df_train[:5].iterrows():\\n\",\n",
    "    \"    filename = row['filename']\\n\",\n",
    "    \"    occlusion = row['FaceOcclusion']\\n\",\n",
    "    \"    gender = row['gender']\\n\",\n",
    "    \"    img2display = Image.open(f\\\"{image_dir}/{filename}\\\")\\n\",\n",
    "    \"    display(img2display)\\n\",\n",
    "    \"    print(filename, occlusion, gender)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1465fe98-f578-4d97-bae1-b1fcaa59183c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Make Dataset and Dataloader\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b580142f-57ae-4a85-9185-340871f46581\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class Dataset(torch.utils.data.Dataset):\\n\",\n",
    "    \"    'Characterizes a dataset for PyTorch'\\n\",\n",
    "    \"    def __init__(self, df, image_dir):\\n\",\n",
    "    \"         'Initialization'\\n\",\n",
    "    \"         self.image_dir = image_dir\\n\",\n",
    "    \"         self.df = df\\n\",\n",
    "    \"         self.transform = transforms.ToTensor()\\n\",\n",
    "    \"         \\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        'Denotes the total number of samples'\\n\",\n",
    "    \"        return len(self.df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __getitem__(self, index):\\n\",\n",
    "    \"        'Generates one sample of data'\\n\",\n",
    "    \"        # Select sample\\n\",\n",
    "    \"        row = self.df.loc[index]\\n\",\n",
    "    \"        filename = row['filename']\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Load data and get label\\n\",\n",
    "    \"        img = Image.open(f\\\"{image_dir}/{filename}\\\")\\n\",\n",
    "    \"        y = row['FaceOcclusion']\\n\",\n",
    "    \"        gender = row['gender']\\n\",\n",
    "    \"\\n\",\n",
    "    \"        X = self.transform(img)\\n\",\n",
    "    \"        y = np.float32(y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return X, y, gender, filename\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"00f9f263-4ae8-4922-b34a-4ce26e80c670\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"training_set = Dataset(df_train, image_dir)\\n\",\n",
    "    \"validation_set = Dataset(df_val, image_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"params_train = {'batch_size': 8,\\n\",\n",
    "    \"          'shuffle': True,\\n\",\n",
    "    \"          'num_workers': 0}\\n\",\n",
    "    \"\\n\",\n",
    "    \"params_val = {'batch_size': 8,\\n\",\n",
    "    \"          'shuffle': False,\\n\",\n",
    "    \"          'num_workers': 0}\\n\",\n",
    "    \"\\n\",\n",
    "    \"training_generator = torch.utils.data.DataLoader(training_set, **params_train)\\n\",\n",
    "    \"validation_generator = torch.utils.data.DataLoader(validation_set, **params_val)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"660e6c36-32e9-4f49-a655-7a4ff474f9df\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Create naive model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5d1400b5-5c75-4512-9b55-3a49b7de99e2\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model = torchvision.models.mobilenet_v3_small(num_classes=1)\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    model.cuda()    \\n\",\n",
    "    \"model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"11c518a3-da5d-4ab4-aafe-17e4a4bbbf0a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Loss and optimizer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"8e6c197d-1b93-405f-a2b3-ead6027e33f1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"loss_fn = nn.MSELoss()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"ef7c0bc4-5415-4121-b22d-cba57ff7fe5f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c317a740-179b-4056-a9c9-fee96bd151c7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Train naive model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"cbf12e7c-c57f-4cac-bcc2-d244404a7d67\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# CUDA for PyTorch\\n\",\n",
    "    \"use_cuda = torch.cuda.is_available()\\n\",\n",
    "    \"device = torch.device(\\\"cuda:0\\\" if use_cuda else \\\"cpu\\\")\\n\",\n",
    "    \"torch.backends.cudnn.benchmark = True\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0aa6829d-8085-4e1a-b3d5-b24be1be83d4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Fit on train split\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f38bfdf0-4dad-42c2-9d28-3cea32e07f94\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"num_epochs = 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"for n in range(num_epochs):\\n\",\n",
    "    \"    print(f\\\"Epoch {n}\\\")\\n\",\n",
    "    \"    for batch_idx, (X, y, gender, filename) in tqdm(enumerate(training_generator), total=len(training_generator)):\\n\",\n",
    "    \"        # Transfer to GPU\\n\",\n",
    "    \"        X, y = X.to(device), y.to(device)\\n\",\n",
    "    \"        y = torch.reshape(y, (len(y), 1))\\n\",\n",
    "    \"        y_pred = model(X)\\n\",\n",
    "    \"        loss = loss_fn(y_pred, y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if loss.isnan():\\n\",\n",
    "    \"            print(filename)\\n\",\n",
    "    \"            print('label', y)\\n\",\n",
    "    \"            print('y_pred', y_pred)\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if batch_idx % 200 == 0:\\n\",\n",
    "    \"            print(loss)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        optimizer.zero_grad()\\n\",\n",
    "    \"        loss.backward()\\n\",\n",
    "    \"        optimizer.step()\\n\",\n",
    "    \"        \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"23d57a8f-eb3b-4e51-9742-e551632a5e1b\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Evaluate metric on validation split\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"63cb13ef-c632-4218-81d1-4044398bcd34\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def error_fn(df):\\n\",\n",
    "    \"    pred = df.loc[:, \\\"pred\\\"]\\n\",\n",
    "    \"    ground_truth = df.loc[:, \\\"target\\\"]\\n\",\n",
    "    \"    weight = 1/30 + ground_truth\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return np.sum(((pred - ground_truth)**2) * weight, axis=0) / np.sum(weight, axis=0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def metric_fn(female, male):\\n\",\n",
    "    \"    err_male = error_fn(male)\\n\",\n",
    "    \"    err_female = error_fn(female)\\n\",\n",
    "    \"    return (err_male + err_female) / 2 + abs(err_male - err_female)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"63d530c4-59ce-4a8b-9936-260c46ef5cea\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results_list = []\\n\",\n",
    "    \"for batch_idx, (X, y, gender, filename) in tqdm(enumerate(validation_generator), total=len(validation_generator)):\\n\",\n",
    "    \"        X, y = X.to(device), y.to(device)\\n\",\n",
    "    \"        y_pred = model(X)\\n\",\n",
    "    \"        for i in range(len(X)):\\n\",\n",
    "    \"\\n\",\n",
    "    \"            results_list.append({'pred': float(y_pred[i]),\\n\",\n",
    "    \"                                  'target': float(y[i]),\\n\",\n",
    "    \"                                  'gender': float(gender[i])\\n\",\n",
    "    \"                                 })\\n\",\n",
    "    \"results_df = pd.DataFrame(results_list)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"53a2ee7d-f0d6-46c3-96e8-8a6f11208b4e\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"4dd58f48-bd46-4d2d-a291-9b6af4e5e57e\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results_df['pred'].hist()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"063209ae-d673-4096-a8eb-487dd14b2ba9\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results_male = results_df.loc[results_df[\\\"gender\\\"] > 0.5]\\n\",\n",
    "    \"results_female = results_df.loc[results_df[\\\"gender\\\"] < 0.5]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1e8ebfae-8101-4b2c-bec5-9b60845f453f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"metric_fn(results_male, results_female)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4006c9fd\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Evaluating your Test DataSet\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d5edd7ce-e687-433c-8248-0dd4d7a6f21a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class Dataset_test(torch.utils.data.Dataset):\\n\",\n",
    "    \"    'Characterizes a dataset for PyTorch'\\n\",\n",
    "    \"    def __init__(self, df, image_dir):\\n\",\n",
    "    \"         'Initialization'\\n\",\n",
    "    \"         self.image_dir = image_dir\\n\",\n",
    "    \"         self.df = df\\n\",\n",
    "    \"         self.transform = transforms.ToTensor()\\n\",\n",
    "    \"         \\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        'Denotes the total number of samples'\\n\",\n",
    "    \"        return len(self.df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __getitem__(self, index):\\n\",\n",
    "    \"        'Generates one sample of data'\\n\",\n",
    "    \"        # Select sample\\n\",\n",
    "    \"        row = self.df.loc[index]\\n\",\n",
    "    \"        filename = row['filename']\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Load data and get label\\n\",\n",
    "    \"        img = Image.open(f\\\"{image_dir}/{filename}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        X = self.transform(img)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return X, filename\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"60cfdfef\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"test_set = Dataset_test(df_test, image_dir)\\n\",\n",
    "    \"\\n\",\n",
    "    \"params_test = {'batch_size': 8,\\n\",\n",
    "    \"          'shuffle': False,\\n\",\n",
    "    \"          'num_workers': 0}\\n\",\n",
    "    \"\\n\",\n",
    "    \"test_generator = torch.utils.data.DataLoader(test_set, **params_test)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9af5bcf6\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"results_list = []\\n\",\n",
    "    \"for batch_idx, (X, filename) in tqdm(enumerate(test_generator), total=len(test_generator)):\\n\",\n",
    "    \"        X = X.to(device)\\n\",\n",
    "    \"        y_pred = model(X)\\n\",\n",
    "    \"        for i in range(len(X)):\\n\",\n",
    "    \"\\n\",\n",
    "    \"            results_list.append({'pred': float(y_pred[i])\\n\",\n",
    "    \"                                 })\\n\",\n",
    "    \"test_df = pd.DataFrame(results_list)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a8dbca68\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"test_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0a67de71\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Preparing your file for Submission\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a6d8c3b3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"test_df.to_csv(\\\"Data_Challenge.csv\\\", header = None, index = None)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"88ff8155\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Now it's your turn. Good luck !  :) \"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
