{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANALYSE DU PROBLEME**\n",
    "---\n",
    "\n",
    "Problème de classification (28 classes) avec évaluation \"sur mesure\" (composite) qui tient compte de la performance et de l'équité du modèle<br><br>. \n",
    "\n",
    "Jeu de donnée de 25.000 personnes, après voir lancé la baseline, j'ai entrainé un **adversarial NN**, qui permettait de gagner un peu d'équité (passage de 80% à 85% sur le TRP gap) mais a entrainé un éffondrement de l'accuracy (de 65% à 35%). après avoir cherché à personaliser la fonction de perte (avec difficulté pour entrainer le modele), je suis repassée à une approche plus progressive, sur la régression logistique\n",
    "\n",
    "**regression logisitique (baseline) => 3 problemes**<br>\n",
    "1. ***l'échantillonage*** : Le \"train_test_split\" ne tient pas compte de S, nous ne savons pas si l'attribut protégé (H/F) est bien réparti entre X_train et X_test, cela n'est pas optimum pour l'apprentissage car certaines classes contiennent peu de points. Il peut aussi etre plus pertinent de fixer les hyperparametre grace à X_train, et d'entrainer le modele sur tout l'échantillon (avec les memes hyperparametres) avant de prédire X_test_true et de soumettre<br>\n",
    "=> nous réaliserons des train_test_split en tenant compte de X et Y grace à Y56 = Y + 28*S <br>\n",
    "=> nous ferons des soumissions double (modele appris sur X_train et aussi X) pour le datachallenge, pour améliorer les qualités prédicitive du modèles <br><br>\n",
    "2. la fonction d'évaluation est une ***moyenne de scores calculé par classes***, chaque classe a donc le meme poids dans le score final. Or, l'apprentissage est fait pour optimser l'accuracy de l'échantillon (chaque élément à le meme poids). Comme la distribution de l'échatillon est très mal équilibrée entre les classes (93 à 8.285 personnes), le modèle ne peut apprendre correctement<br>\n",
    "=> nous essaierons pour la forme del a data augmentation, mais sans conviction compte tenu de l'embedding sémantique fait avec BERT<br>\n",
    "=> nous utiliserons une correction de la fonction de pertes pour y inclure le poids de chaque classe (Y et Y56)<br>\n",
    "=> nous essaierons du contrastive learning pour obtenir un espace de réprésentation adéquat<br><br>\n",
    "2. ***l'apprentissage n'est pas optimal (alignement et hyperparametres à améliorer)***. Il manque la régularization et du cross validation. L'apprentissage ,'est pas aligné avec \"score final\": le modele fait pour optimiser l'accuracy (la précision), or la fonction d'évaluation est une moyenne entre le F1 score (moyenne harmonique de la précision et du rappel) et une métrique de fairness. Le F1 score pénalise les écarts importants entre la précision et le rappel. Le score<br>\n",
    "=> nous adapterons le fonctions de pertes et/ou l'architecture des modèles intégrer au mieux notrescore final\n",
    "=> nous ajouterons systematiquement des termes de régularisations et de cross validation (stratified k-fold cross validation si possibl)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHODOLOGIE**\n",
    "---\n",
    "\n",
    "nous **ajusterons la fonction de pertes**, pour qu'elle tienne compte au mieux de la fonction d'évaluation, qui est composée de 2 éléments : le F1 score et le 1-equal ooprtunity gap<br>\n",
    "- le 'macro f score' (F1 score) n'est pas dérivable (à valider), nous l'approcherons donc par accuracy et le recall<br>\n",
    "- true positive gap : 1 - TRP_GAP est calculé comme \n",
    "A noter que le F1 score n'est pas dérivable. However, we can approximate the F1 score in a differentiable manner by using the ***Sørensen–Dice coefficient***, which is closely related to the F1 score and is differentiable. The Sørensen–Dice coefficient is defined as \\(2 * \\frac{precision * recall}{precision + recall}\\), which is equivalent to the F1 score formula. For a differentiable approximation, we can use soft versions of precision and recall.\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Import, loading data, eval functions**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_val_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import make_scorer #, f1_score\n",
    "\n",
    "#from Data_challenge_fairness_2024.evaluator import *\n",
    "from evaluator import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_model = 'BIAS_models/'  # must finish by '/'\n",
    "path_Y_pred_true = 'BIAS_Y_pred_true/'  # must finish by '/'\n",
    "\n",
    "# with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: model = pickle.load(f)\n",
    "\n",
    "def get_final_score(Y_pred, Y, S):\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "    return final_score\n",
    "    \n",
    "    \n",
    "def get_scores(Y_pred, Y, S, will_print=1):\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    if will_print==1 : print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    \n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "    macro_f1 = eval_scores['macro_fscore']\n",
    "    inv_macro_gap = 1-eval_scores['TPR_GAP']\n",
    "\n",
    "    if will_print==1: #print results\n",
    "        print('final score :',final_score)\n",
    "        print('macro_f1    :',eval_scores['macro_fscore'])\n",
    "        print('inv_macro_gap',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, macro_f1, inv_macro_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['X_train', 'X_test', 'Y', 'S_train', 'S_test'])\n",
      "(27749, 768) (27749,) (27749,) (11893, 768) (11893,)\n"
     ]
    }
   ],
   "source": [
    "#####################################################\n",
    "# Load pickle file and convert to numpy array\n",
    "#####################################################\n",
    "\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)\n",
    "\n",
    "#Check keys()\n",
    "print(dat.keys())\n",
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']\n",
    "\n",
    "X_test = dat['X_test']\n",
    "S_test = dat['S_test']\n",
    "\n",
    "Y56= Y + 28*S\n",
    "#X, X_test,Y,S, S_test = dat[1]\n",
    "\n",
    "print(X.shape,Y.shape,S.shape,X_test.shape,S_test.shape)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val, S_train, S_val = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "\n",
    "path_model_ = 'BIAS_models/'\n",
    "path_Y_pred = 'BIAS_Y_pred/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL 6 - ADPAPTING LOSS FUNCTION IN LOGISTIC REGRESSION (PYTORCH)**\n",
    "---\n",
    "1. Functions for Custom loss function (re-written in pytorch)\n",
    "2. Functions for Stratified K-fold and training batches\n",
    "3. Running Model 6\n",
    "4. Optimising Model 6 hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PROBLEME SUR FINAL SCORE QUI DEPEND DE S, OR SICKIT LEARN NE GERE\n",
    "PAS S DANS LA CROSS VALIDATION, LA LOSS FUNCTION PERSONALISEE NE PEUT \n",
    "ETRE CALCULEE  =======> CODAGE SUR PYTORCH\n",
    "\n",
    "##########################################################################\n",
    "# 5. CUSTOM LOSS FUNCTION (Using stratified K + custom_scorer)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "name ='5_Reglog_StratKFold1_Orthogonal_CustomLoss' # changer clf_i\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF CUSTOM LOSS FUNCTION (FINAL SCORE) \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "#It is not possible to split S with the stratified k-fold implementation \n",
    "# in sickit learn. We will use Y56 = Y + 28*S to train logistic model\n",
    "# and derive from Y56 the value of Y = Y56 % 28 and S = Y56//28\n",
    "\n",
    "def final_score_S(Y,Y_pred):\n",
    "    # wrapper function to include S\n",
    "    # Note order : custom scorer expects (y_true, y_pred) as inputs\n",
    "    \n",
    "    return get_final_score(Y_pred,Y,S)\n",
    "\n",
    "custom_scorer = make_scorer(final_score_S)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "clf_5 = LogisticRegression(random_state=0, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=1)\n",
    "scores = cross_val_score(clf_3, debiased_X, Y, scoring=custom_scorer ,cv=cv, n_jobs=-1)\n",
    "clf_5.fit(debiased_X, Y56)\n",
    "model = clf_5\n",
    "# with open(path_model + name + '.pkl', 'rb') as f: clf_5 = pickle.load(f)\n",
    "\n",
    "# predicting and assessing\n",
    "Y56_pred = model.predict(X_test)\n",
    "#S_pred = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_pred,Y_test,S_test)\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test_true = remove_info(X_test_true, bias)  # debiasing\n",
    "Y56_pred_true = model.predict(modified_X_test_true)\n",
    "Y_pred_true = Y56_pred_true % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "results=pd.DataFrame(Y_pred_true, columns= ['score'])\n",
    "results.to_csv(path_Y_pred_true + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on transformed test data: 1.0\n",
      "final score : 0.9999995472208593\n",
      "macro_f1    : 1.0\n",
      "inv_macro_gap 0.9999990944417185\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# 5. CUSTOM LOSS FUNCTION WITH 56 CLASS CLASSIFIER (USING Y56 = Y + S*28)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "name ='5_Reglog56_StratKFold_Orthogonal_CustomLoss' # changer clf_i\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Debiasing X (Projection of X orthogonally to bias) \n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# projection in new representation space\n",
    "debiased_X = remove_bias(X, bias)\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# CREATION OF CUSTOM LOSS FUNCTION (FINAL SCORE) \n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "#It is not possible to split S with the stratified k-fold implementation \n",
    "# in sickit learn. We will use Y56 = Y + 28*S to train logistic model\n",
    "# and derive from Y56 the value of Y = Y56 % 28 and S = Y56//28\n",
    "\n",
    "def final_score_S(Y56,Y_pred):\n",
    "    '''custom scorer expects (y_true, y_pred) as inputs\n",
    "    Inputs : Y56 and Y pred\n",
    "    Outputs : final score on 28 classes with Y56 trick (S=Y56//28 and Y=Y56%28)'''\n",
    "    S = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "    Y = Y56 % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "    return get_final_score(Y_pred,Y,S)\n",
    "\n",
    "custom_scorer = make_scorer(final_score_S)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TRAINING MODEL AND PREDICTING\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "'''BlockingIOErrorclf_5 = LogisticRegression(random_state=42, max_iter=5000,verbose=0,penalty='l2', C=0.2, tol=0.0001)\n",
    "\n",
    "# Préparer la procédure de validation croisée k-fold stratifiée\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores = cross_val_score(clf_5, debiased_X, Y56, scoring=custom_scorer ,cv=cv, n_jobs=-1)\n",
    "clf_5.fit(debiased_X, Y56)'''\n",
    "with open(path_model + name + '.pkl', 'rb') as f: clf_5 = pickle.load(f)\n",
    "model = clf_5\n",
    "\n",
    "# predicting and assessing\n",
    "Y56_pred = model.predict(X_val)\n",
    "#S_pred = Y56//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "Y_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "accuracy, final_score, macro_f1, inv_macro_gap = get_scores(Y_val,Y_val,S_val)\n",
    "\n",
    "\n",
    "# predict X_test_true and save\n",
    "modified_X_test = remove_bias(X_test, bias)  # debiasing\n",
    "Y56_pred = model.predict(modified_X_test)\n",
    "Y_pred = Y56_pred % 28  # reste (original Y)   ex 33% 28 = classe 5\n",
    "results=pd.DataFrame(Y_pred, columns= ['score'])\n",
    "#results.to_csv(path_Y_pred + \"Data_Challenge_\" + name + \".csv\", header = None, index = None)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 6. CUSTOM LOSS FUNCTION IN PYTORCH (CLASSIFIER ON 28 ORIGINAL CLASSES)\n",
    "# + Debiasing with orthogonal projection + Optimised Logisitic Regression\n",
    "##########################################################################\n",
    "\n",
    "name ='5_Reglog56_StratKFold_Orthogonal_CustomLoss' # changer clf_i\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "#     CUSTOM LOSS FUNCTION AND EVALUATION FUNCTIONS\n",
    "#         (RE-WRITTEN FOR PYTORCH)\n",
    "#\n",
    "#   soft_f1_loss\n",
    "#   macro_soft_f1_loss\n",
    "#   get_macro_f1\n",
    "#   get_tpr_gap\n",
    "#   get_macro_tpr_gap\n",
    "#   soft_final_score_loss\n",
    "#   get_final_score\n",
    "#\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "def soft_macro_f1_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the macro F1 score as a loss function.\n",
    "    Calculates the F1 score for each class independently and then takes the average.\n",
    "    Inputs :\n",
    "        y_true must be one hot encoded\n",
    "    \"\"\"\n",
    "    y_pred_one_hot = torch.nn.functional.one_hot(y_pred, num_classes=Y_train.nunique()) if len(y_pred.shape) == 1 else y_pred\n",
    "    #y_pred_probs = torch.softmax(y_pred_one_hot, dim=1)\n",
    "    \n",
    "    tp = torch.sum(y_true * y_pred, dim=0)\n",
    "    pp = torch.sum(y_pred, dim=0)\n",
    "    ap = torch.sum(y_true, dim=0)\n",
    "    \n",
    "    precision = tp / (pp + 1e-6)\n",
    "    recall = tp / (ap + 1e-6)\n",
    "    \n",
    "    f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "    macro_f1 = torch.mean(f1_per_class)   # Mean to aggregate over all classes\n",
    "    \n",
    "    loss = 1 - macro_f1  # Minimizing loss is maximizing macro F1 score\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_macro_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the exact macro F1 score for evaluation.\n",
    "    Input : any format as tensors will be converted to Tensors of true label if dim >1 . Can be :\n",
    "        - Tensor of probabilities(y_pred_probs) dimension (n,28)\n",
    "        - Tensor of labels, one hote encoded (y_pred_one_hot) dimension (n,28)\n",
    "        - Tensor of labels (y_pred_tensor) dimension (n,1)\n",
    "    Ouput : scalar\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    \" predict macro f1\"\n",
    "    f1 = f1_score(y_true_labels.cpu().numpy(), y_pred_labels.cpu().numpy(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def get_tpr_gap(y_true, y_pred, protected_attribute, class_idx):\n",
    "    \"\"\"\n",
    "    Calculate the TPR gap for a specific class across protected groups.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred_probs: Tensor of predicted probabilities (after softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    - class_idx: Index of the class for which to calculate the TPR gap.\n",
    "    \n",
    "    Returns:\n",
    "    - TPR gap for the specified class.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Calculate overall TPR for the current class\n",
    "    overall_mask = y_true_labels == class_idx\n",
    "    overall_tpr = torch.sum((y_pred_labels == class_idx) & overall_mask).float() / (torch.sum(overall_mask).float() + 1e-6)\n",
    "    \n",
    "    # Initialize list to store TPR for each protected group\n",
    "    group_tprs = []\n",
    "    \n",
    "    # Calculate TPR for each protected group\n",
    "    for group_val in protected_attribute.unique():\n",
    "        group_mask = (protected_attribute == group_val) & overall_mask\n",
    "        group_tpr = torch.sum((y_pred_labels == class_idx) & group_mask).float() / (torch.sum(group_mask).float() + 1e-6)\n",
    "        group_tprs.append(group_tpr)\n",
    "    \n",
    "    # Calculate TPR gap for the current class\n",
    "    tpr_gaps = torch.abs(torch.tensor(group_tprs) - overall_tpr)\n",
    "    \n",
    "    return torch.mean(tpr_gaps)  # Return the mean TPR gap for this class\n",
    "\n",
    "def get_macro_tpr_gap(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Calculate the average TPR gap per class by calling tpr_gap for each class.\n",
    "    \n",
    "    Args:\n",
    "    - y_true: Tensor of true labels, one-hot encoded.\n",
    "    - y_pred: Tensor of predicted logits (before softmax).\n",
    "    - protected_attribute: Tensor indicating group membership for each instance.\n",
    "    \n",
    "    Returns:\n",
    "    - Average TPR gap across all classes.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "    \n",
    "    # Initialize list to store TPR gaps for all classes\n",
    "    class_tpr_gaps = []\n",
    "    \n",
    "    # Iterate over each class\n",
    "    num_classes = len(y_true_labels.unique())\n",
    "    for class_idx in range(num_classes):\n",
    "        class_tpr_gap = get_tpr_gap(y_true_labels, y_pred_labels, protected_attribute, class_idx)\n",
    "        class_tpr_gaps.append(class_tpr_gap)\n",
    "    \n",
    "    # Calculate the average TPR gap across all classes\n",
    "    avg_tpr_gap = torch.mean(torch.stack(class_tpr_gaps))\n",
    "    \n",
    "    return avg_tpr_gap\n",
    "\n",
    "\n",
    "def soft_final_score_loss(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    soft_macro_f1 = soft_macro_f1_loss(y_true, y_pred)  # Calculate soft macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true, y_pred, protected_attribute)  # Calculate TPR gap\n",
    "    \n",
    "    soft_final_score = ( soft_macro_f1 + (1 - macro_tpr_gap) ) / 2\n",
    "    return soft_final_score\n",
    "\n",
    "def get_final_score(y_true, y_pred, protected_attribute):\n",
    "    \"\"\"\n",
    "    Combine soft macro F1 score and TPR gap to create a final evaluation metric.\n",
    "    \"\"\"\n",
    "    #convert Tensors to 1 dimension (labels ranging from 0 to 27) if necessary\n",
    "    y_pred_labels = torch.argmax(y_pred, dim=1) if y_pred.ndim > 1 else y_pred\n",
    "    y_true_labels = torch.argmax(y_true, dim=1) if y_true.ndim > 1 else y_true\n",
    "\n",
    "    macro_f1 = get_macro_f1(y_true_labels, y_pred_labels)  # Calculate macro F1 score\n",
    "    macro_tpr_gap = get_macro_tpr_gap(y_true_labels, y_pred_labels, protected_attribute)  # Calculate macro TPR gap\n",
    "    \n",
    "    final_score = (macro_f1 + (1 - macro_tpr_gap)) / 2\n",
    "    return final_score\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# TORCH EVALUATION FUNCTIONS\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def evaluate(Y_pred,Y,S,will_print=1):\n",
    "    '''returns model accuracy, final score, macro fscore ans TPR gap\n",
    "    input : 2 np arrays of same dimension\n",
    "    output : array of 4 values\n",
    "    '''\n",
    "    accuracy= accuracy_score(Y, Y_pred)  # Y_test are your original test labels\n",
    "    print(f\"Accuracy on transformed test data: {accuracy}\")\n",
    "    eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y, S, metrics=['TPR'])\n",
    "    final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "\n",
    "    if will_print==1:\n",
    "        #print results\n",
    "        print('final score',final_score)\n",
    "        print('macro_fscore',eval_scores['macro_fscore'])\n",
    "        print('1-eval_scores[\\'TPR_GAP\\']',1-eval_scores['TPR_GAP'])\n",
    "    \n",
    "    return accuracy, final_score, eval_scores['macro_fscore'],1-eval_scores['TPR_GAP'] , eval_scores , confusion_matrices_eval\n",
    "\n",
    "# to predict X_test and save to file\n",
    "\n",
    "def save_Y_pred_tofile(X, model, name): # adapted to torch\n",
    "    \n",
    "    # save probabilities for each Xi (dim=28)\n",
    "    y_pred_probs = model(X)\n",
    "    probs=pd.DataFrame(y_pred_probs.detach().numpy(), columns= list(range(0,28)))\n",
    "    file_name_probs = \"y_pred_probs/y_pred_probs_\"+str(name)+\".csv\"\n",
    "    probs.to_csv(file_name_probs, header = None, index = None)\n",
    "\n",
    "    # save predicted labels for each Xi (dim=1)\n",
    "    y_pred = torch.argmax(y_pred_probs, dim=1)\n",
    "    results=pd.DataFrame(y_pred.numpy(), columns= ['score'])\n",
    "    file_name = \"y_pred/Data_Challenge_\"+str(name)+\".csv\"\n",
    "    results.to_csv(file_name, header = None, index = None)\n",
    "\n",
    "    return y_pred, y_pred_probs\n",
    "    \n",
    "\n",
    "def print_cassif_report(Y_pred,Y_test):\n",
    "    # Convert Y_pred to a DataFrame\n",
    "    Y_pred_df = pd.DataFrame(Y_pred_tensor.numpy(), columns=['Predicted'])\n",
    "\n",
    "    # Evaluate Y_pred compared to Y_test (assuming Y_test is a numpy array or a pandas Series)\n",
    "    table = classification_report(Y_test, Y_pred_df['Predicted'])\n",
    "\n",
    "    return table\n",
    "\n",
    "#########################################################################\n",
    "#     ENTRAINEMENT DU MODEL (AVEC MINI BATCH / SANS CROSS VALIDATION)\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "def train_NN_with_custom_loss(model, optimizer, batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor):\n",
    "\n",
    "    # 1. Convertir les tensors en datasets puis en DataLoader pour gérer les mini-batchs\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_one_hot, S_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_one_hot, S_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    # 2. Paramètres pour l'arrêt précoce\n",
    "    # -----------------------------------\n",
    "    patience = 5  # Nombre d'époques à attendre après la dernière amélioration de la perte de validation\n",
    "    best_loss = None\n",
    "    early_ending = None\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 1/ exécuter les minibatches et recupérer la loss moyenne\n",
    "        for X_batch, Y_batch, S_batch in train_loader:\n",
    "            # Y_batch est one hot\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs_train = model(X_batch)\n",
    "            loss = soft_final_score_loss(Y_batch, outputs_train, S_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # save mini-batch loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Average loss pour l'epoch (après boucle mini-batchs)\n",
    "        train_loss = train_loss / len(train_loader)       \n",
    "        \n",
    "        # 2. Vérifier si la perte de validation s'est améliorée (arret précoce)\n",
    "\n",
    "        # Evaluation sur le jeu de données de test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch_test, Y_batch_test, S_batch_test in test_loader:\n",
    "                outputs_test = model(X_batch_test)\n",
    "                #Y_batch_test_one_hot = torch.nn.functional.one_hot(Y_batch_test, num_classes=Y_train.nunique())\n",
    "                loss_test = soft_final_score_loss(Y_batch_test, outputs_test, S_batch_test)\n",
    "                test_loss += loss_test.item()\n",
    "                \n",
    "        #average_test_loss = running_loss_test / len(test_loader)\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "       \n",
    "        # check if improvement in loss (compared to last epoch)\n",
    "        if best_loss is None or test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Arrêt précoce après {epoch+1} époques')\n",
    "                early_ending = epoch + 1\n",
    "                break  # Arrêter l'entraînement\n",
    "        \n",
    "        # 3. Impression de l'apprentissage et des scores train et test\n",
    "        if epoch==0 or (epoch+1) % 10 == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Calculate metrics for training data\n",
    "                outputs_train = model(X_train_tensor) # probabilities\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_train_ = get_final_score(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "                macro_f1_train = get_macro_f1(Y_train_tensor, outputs_train)\n",
    "                inv_macro_tpr_gap_train = 1 - get_macro_tpr_gap(Y_train_tensor, outputs_train, S_train_tensor)\n",
    "            \n",
    "                # Calculate metrics for test data\n",
    "                outputs_test = model(X_test_tensor)\n",
    "                # Evaluate predictions on training data\n",
    "                final_score_test_ = get_final_score(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "                macro_f1_test = get_macro_f1(Y_test_tensor, outputs_test)\n",
    "                inv_macro_tpr_gap_test = 1 - get_macro_tpr_gap(Y_test_tensor, outputs_test, S_test_tensor)\n",
    "\n",
    "                print(f'Epoch {epoch+1}, Loss: {loss.item()}, Final Score Train: {final_score_train_.item()}, Final Score Test: {final_score_test_.item()} (gap {final_score_test_-final_score_train_}) macro F1 Train: {macro_f1_train}, macro F1 Test: {macro_f1_test}, 1-TPR Gap Train: {inv_macro_tpr_gap_train}, 1-TPR Gap Test: {inv_macro_tpr_gap_test}')\n",
    "            \n",
    "    # 4. Make Predictions and Evaluate with final_score\n",
    "    # -------------------------------------------------\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        Y_train_pred_probs = model(X_train_tensor) # dim = 28 (Probabilities for each class)\n",
    "        \n",
    "        # # Y_train_pred_tensor = torch.argmax(Y_train_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        final_score_train = get_final_score(Y_train_tensor, Y_train_pred_probs, S_train_tensor)\n",
    "\n",
    "        Y_pred_probs = model(X_test_tensor) # dim = 28 (Probabilities for each class)\n",
    "        Y_pred_tensor = torch.argmax(Y_pred_probs, dim=1)  # dim = 1 (Get the class with the highest probability)\n",
    "        macro_f1 = get_macro_f1(Y_test_tensor, Y_pred_tensor)\n",
    "        inv_macro_tpr_gap = 1 - get_macro_tpr_gap(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        final_score = get_final_score(Y_test_tensor, Y_pred_probs, S_test_tensor)\n",
    "        \n",
    "        print(f'Final Evaluation Score: {final_score.item()} gap {final_score.item()-final_score_train.item()} || Macro F1: {macro_f1.item()} 1-TPR_gap: { inv_macro_tpr_gap.item() }')\n",
    "\n",
    "    return model, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending,final_score_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# train_test_split (np.arrays)\n",
    "##############################################################\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, Y56_train, Y56_test = train_test_split(debiased_X, Y56, test_size=0.2, random_state=42)\n",
    "\n",
    "Y_train = Y56_train % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_train = Y56_train//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "Y_test = Y56_test % 28  # reste (original Y)   ex 33% 28 = classe 5 \n",
    "S_test = Y56_test//28   # facteur (original S) ex 33//28 = 1 (attribut protégé)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train:',X_train.shape,Y_train.shape,S_train.shape)\n",
    "print('test:',X_test.shape,Y_test.shape, S_test.shape)\n",
    "\n",
    "##############################################################\n",
    "# 1. Transform DataFrames into Tensors\n",
    "##############################################################\n",
    "\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y.values, dtype=torch.long)\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.long)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train.values, dtype=torch.long)\n",
    "S_train_tensor = torch.tensor(S_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.long)\n",
    "S_test_tensor = torch.tensor(S_test.values, dtype=torch.long)\n",
    "\n",
    "Y_train_one_hot = torch.nn.functional.one_hot(Y_train_tensor, num_classes=Y_train.nunique())\n",
    "Y_test_one_hot = torch.nn.functional.one_hot(Y_test_tensor, num_classes=Y_train.nunique())\n",
    "\n",
    "X_test_true_tensor = torch.tensor(X_test_true.values, dtype=torch.float32)\n",
    "#S_test_true_tensor = torch.tensor(S_test_true.values, dtype=torch.long)\n",
    "\n",
    "# impression des dimensions\n",
    "print('train_tensor:',X_train_tensor.shape,Y_train_tensor.shape,S_train_tensor.shape, type(X_train_tensor))\n",
    "print('test_tensor:',X_test_tensor.shape,Y_test_tensor.shape, S_test_tensor.shape, type(X_test_tensor))\n",
    "print('Y_train_one_hot:',Y_train_one_hot.shape, type(Y_train_one_hot))\n",
    "print('X_test_true_tensor:',X_test_true_tensor.shape, type(X_test_true_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#       TEST D'UN MODEL (ET DU CODE)\n",
    "################################################\n",
    "\n",
    "\n",
    "# 1. Define the model and optimizer and train\n",
    "# --------------------------------------------------\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 28),  # Assuming 768 input features and 28 classes\n",
    "    nn.ReLU(),  # Adding a ReLU activation function\n",
    "    nn.Linear(28, 28),\n",
    "    nn.Softmax(dim=1),  # LogSoftmax for multi-class classification\n",
    "    )  \n",
    "\n",
    "batch_size = 512\n",
    "learning_rate=0.001\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=1/0.2)\n",
    "num_epochs = 1000\n",
    "\n",
    "# 2. Train the model with the custom loss function final_eval\n",
    "# -----------------------------------------------------------\n",
    "name = 'NN-28-28_Adam'+'_lr_'+str(learning_rate)+'_batch_size_'+str(batch_size)\n",
    "print('\\n\\n Starting to train model', name)\n",
    "model_trained, Y_pred_probs, Y_pred_tensor, final_score, macro_f1, inv_macro_tpr_gap, early_ending, final_score_train = train_NN_with_custom_loss(model,optim.Adam(model.parameters(), lr=learning_rate), batch_size, X_train_tensor, Y_train_tensor, S_train_tensor, X_test_tensor, Y_test_tensor, S_test_tensor)\n",
    "\n",
    "# Res=pd.DataFrame(columns=['model','optimizer','lr','batch_size','early_ending', 'final_score_train','final_score','macro_f1','macro_tpr_gap'])\n",
    "# Res.loc[i]=[name,optimizer,learning_rate,batch_size, early_ending,final_score_train, final_score, macro_f1, inv_macro_tpr_gap]\n",
    "\n",
    "# Save predictions on X_test_true\n",
    "save_Y_pred_tofile(X_test_true_tensor, model_trained,name)\n",
    "\n",
    "# save model\n",
    "with open(path_model + name + '.pkl', 'wb') as f: pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python WSL (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
